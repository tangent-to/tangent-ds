{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Patterns in tangent/ds\n",
    "\n",
    "The tangent/ds package supports three different API patterns, each designed for different use cases and preferences. This notebook explains when and how to use each pattern.\n",
    "\n",
    "## The three API patterns\n",
    "\n",
    "1. **Array API**: Pure numeric arrays, inspired by NumPy/SciPy\n",
    "2. **Declarative Table API**: Column-based data with metadata, inspired by R and pandas\n",
    "3. **Recipe API**: Chainable preprocessing workflows with full inspection\n",
    "\n",
    "All three patterns are available across the package: MVA (PCA, LDA, RDA), Stats (GLM), and ML (preprocessing, estimators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───────┬──────────┬─────────────┬──────────────────┬─────────────────┬─────────────────────┬───────────────┬──────────┐\n",
      "│ (idx) │ Species  │ Island      │ Beak Length (mm) │ Beak Depth (mm) │ Flipper Length (mm) │ Body Mass (g) │ Sex      │\n",
      "├───────┼──────────┼─────────────┼──────────────────┼─────────────────┼─────────────────────┼───────────────┼──────────┤\n",
      "│     0 │ \u001b[32m\"Adelie\"\u001b[39m │ \u001b[32m\"Torgersen\"\u001b[39m │ \u001b[33m39.1\u001b[39m             │ \u001b[33m18.7\u001b[39m            │ \u001b[33m181\u001b[39m                 │ \u001b[33m3750\u001b[39m          │ \u001b[32m\"MALE\"\u001b[39m   │\n",
      "│     1 │ \u001b[32m\"Adelie\"\u001b[39m │ \u001b[32m\"Torgersen\"\u001b[39m │ \u001b[33m39.5\u001b[39m             │ \u001b[33m17.4\u001b[39m            │ \u001b[33m186\u001b[39m                 │ \u001b[33m3800\u001b[39m          │ \u001b[32m\"FEMALE\"\u001b[39m │\n",
      "│     2 │ \u001b[32m\"Adelie\"\u001b[39m │ \u001b[32m\"Torgersen\"\u001b[39m │ \u001b[33m40.3\u001b[39m             │ \u001b[33m18\u001b[39m              │ \u001b[33m195\u001b[39m                 │ \u001b[33m3250\u001b[39m          │ \u001b[32m\"FEMALE\"\u001b[39m │\n",
      "│     3 │ \u001b[32m\"Adelie\"\u001b[39m │ \u001b[32m\"Torgersen\"\u001b[39m │ \u001b[33m36.7\u001b[39m             │ \u001b[33m19.3\u001b[39m            │ \u001b[33m193\u001b[39m                 │ \u001b[33m3450\u001b[39m          │ \u001b[32m\"FEMALE\"\u001b[39m │\n",
      "│     4 │ \u001b[32m\"Adelie\"\u001b[39m │ \u001b[32m\"Torgersen\"\u001b[39m │ \u001b[33m39.3\u001b[39m             │ \u001b[33m20.6\u001b[39m            │ \u001b[33m190\u001b[39m                 │ \u001b[33m3650\u001b[39m          │ \u001b[32m\"MALE\"\u001b[39m   │\n",
      "└───────┴──────────┴─────────────┴──────────────────┴─────────────────┴─────────────────────┴───────────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "// our base library\n",
    "import * as ds from '../../src/index.js';\n",
    "\n",
    "// data\n",
    "const penguinsResponse = await fetch(\n",
    "  'https://cdn.jsdelivr.net/npm/vega-datasets@2/data/penguins.json',\n",
    ");\n",
    "const penguinsDataRaw = await penguinsResponse.json();\n",
    "\n",
    "// There is a data with a . instead of null...\n",
    "const penguinsData = penguinsDataRaw\n",
    "  .map(row => row.Sex === '.' ? { ...row, Sex: null } : row)\n",
    "  .filter(row => row.Sex);\n",
    "\n",
    "console.table(penguinsData.slice(0, 5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array API\n",
    "\n",
    "The Array API works with pure numeric matrices (arrays of arrays). This is the most direct approach for users familiar with NumPy, MATLAB, or scikit-learn. The array API is fed by arrays, and also returns arrays. You might prefer to use it when\n",
    "\n",
    "- you already have numeric data in array format,\n",
    "- you want minimal abstraction and maximum performance,\n",
    "- you want to remove missing values yourself,\n",
    "- you're translating code from Python/NumPy/scikit-learn or\n",
    "- you don't need column names or metadata.\n",
    "\n",
    "Here is an example to perform a PCA with the array API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    pc1: \u001b[33m-0.061304584756479916\u001b[39m,\n",
      "    pc2: \u001b[33m-0.0019922594730998333\u001b[39m,\n",
      "    pc3: \u001b[33m-0.02120123982548083\u001b[39m,\n",
      "    pc4: \u001b[33m0.0881572884490574\u001b[39m\n",
      "  },\n",
      "  {\n",
      "    pc1: \u001b[33m-0.04353296961059291\u001b[39m,\n",
      "    pc2: \u001b[33m0.027553401236946346\u001b[39m,\n",
      "    pc3: \u001b[33m-0.00247933397270733\u001b[39m,\n",
      "    pc4: \u001b[33m0.06702376349110455\u001b[39m\n",
      "  },\n",
      "  {\n",
      "    pc1: \u001b[33m-0.04552898224857241\u001b[39m,\n",
      "    pc2: \u001b[33m0.01001618930277428\u001b[39m,\n",
      "    pc3: \u001b[33m0.017120558232805\u001b[39m,\n",
      "    pc4: \u001b[33m-0.08820154693715927\u001b[39m\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "const numericColumns = [\n",
    "  \"Beak Length (mm)\",\n",
    "  \"Beak Depth (mm)\",\n",
    "  \"Flipper Length (mm)\",\n",
    "  \"Body Mass (g)\"\n",
    "];\n",
    "\n",
    "// Keep Array API input aligned with Table API (naOmit=true drops rows with missing values).\n",
    "const penguinsArray = penguinsData\n",
    "  .filter(d => numericColumns.every(col => d[col] != null))\n",
    "  .map(d => numericColumns.map(col => d[col]));\n",
    "\n",
    "const pca = ds.mva.pca.fit(penguinsArray, { scale: true, center: true });\n",
    "const scores = ds.mva.pca.transform(pca, penguinsArray);\n",
    "console.log(scores.slice(0, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how scaling would work with the array API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scaled data (first sample): [\n",
      "  \u001b[33m-0.8960418897725946\u001b[39m,\n",
      "  \u001b[33m0.7807321043966294\u001b[39m,\n",
      "  \u001b[33m-1.426751567213706\u001b[39m,\n",
      "  \u001b[33m-0.5684747832140795\u001b[39m\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "const penguinsScaler = new ds.ml.preprocessing.StandardScaler().fit(penguinsArray);\n",
    "const penguinsScaled = penguinsScaler.transform(penguinsArray);\n",
    "console.log('\\nScaled data (first sample):', penguinsScaled[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning can also be approached with arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "const species = penguinsData.map(d => d.Species);\n",
    "\n",
    "// trainTestSplit with Array API\n",
    "const penguinsSplit = ds.ml.validation.trainTestSplit(\n",
    "  penguinsArray,\n",
    "  species,\n",
    "  { ratio: 0.7, shuffle: true, seed: 42 }\n",
    ");\n",
    "\n",
    "// Fit scaler on training data only\n",
    "const penguinsScaler = new ds.ml.preprocessing.StandardScaler()\n",
    "  .fit(penguinsSplit.XTrain);\n",
    "\n",
    "// Transform train and test separately\n",
    "const XTrainScaled = penguinsScaler.transform(penguinsSplit.XTrain);\n",
    "const XTestScaled = penguinsScaler.transform(penguinsSplit.XTest);\n",
    "\n",
    "// Fit classifier\n",
    "const knn = new ds.ml.KNNClassifier({ k: 10 });\n",
    "knn.fit(XTrainScaled, penguinsSplit.yTrain);\n",
    "\n",
    "// Predict\n",
    "const predictions = knn.predict(XTestScaled);\n",
    "undefined;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declarative table API\n",
    "\n",
    "The Table API works with structured data (arrays of objects) and column selectors. This approach preserves column names, handles missing values, and maintains metadata throughout the analysis. It was designed in the perspective where\n",
    "\n",
    "- your data is already in table format (CSV, JSON, database),\n",
    "- you want to preserve column names and metadata,\n",
    "- you need automatic handling of mixed data types,\n",
    "- you prefer longer but declarative, self-documenting code, or\n",
    "- you're familiar with more tidy approaches in R, pandas, or SQL.\n",
    "\n",
    "The table API is fed with data (Arrays of objects or Arquero), and outputs objects with data, column names, and metadata. It automates common tasks.\n",
    "\n",
    "Let's try it for PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ReferenceError",
     "evalue": "ds is not defined",
     "output_type": "error",
     "traceback": [
      "Stack trace:",
      "ReferenceError: ds is not defined",
      "    at <anonymous>:1:39"
     ]
    }
   ],
   "source": [
    "const pcaTable = ds.mva.pca.fit({\n",
    "  data: penguinsData,\n",
    "  columns: numericColumns,\n",
    "  scale: true,\n",
    "  center: true\n",
    "});\n",
    "console.table(pcaTable.scores.slice(0, 3));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same scaling operation as before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scaled data (first sample): {\n",
      "  Species: \u001b[32m\"Adelie\"\u001b[39m,\n",
      "  Island: \u001b[32m\"Torgersen\"\u001b[39m,\n",
      "  \u001b[32m\"Beak Length (mm)\"\u001b[39m: \u001b[33m-0.8960418897725946\u001b[39m,\n",
      "  \u001b[32m\"Beak Depth (mm)\"\u001b[39m: \u001b[33m0.7807321043966294\u001b[39m,\n",
      "  \u001b[32m\"Flipper Length (mm)\"\u001b[39m: \u001b[33m-1.426751567213706\u001b[39m,\n",
      "  \u001b[32m\"Body Mass (g)\"\u001b[39m: \u001b[33m-0.5684747832140795\u001b[39m,\n",
      "  Sex: \u001b[32m\"MALE\"\u001b[39m\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const scalerTable = new ds.ml.preprocessing.StandardScaler();\n",
    "scalerTable.fit({\n",
    "  data: penguinsData,\n",
    "  columns: numericColumns,\n",
    "\n",
    "});\n",
    "const scaledTable = scalerTable.transform({\n",
    "  data: penguinsData,\n",
    "  columns: numericColumns,\n",
    "});\n",
    "console.log('\\nScaled data (first sample):', scaledTable.data[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [\n",
      "  \u001b[32m\"Chinstrap\"\u001b[39m, \u001b[32m\"Gentoo\"\u001b[39m,    \u001b[32m\"Gentoo\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Chinstrap\"\u001b[39m,\n",
      "  \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Gentoo\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Gentoo\"\u001b[39m,\n",
      "  \u001b[32m\"Chinstrap\"\u001b[39m, \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,\n",
      "  \u001b[32m\"Chinstrap\"\u001b[39m, \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,\n",
      "  \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Gentoo\"\u001b[39m,\n",
      "  \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Gentoo\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,\n",
      "  \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Chinstrap\"\u001b[39m, \u001b[32m\"Chinstrap\"\u001b[39m, \u001b[32m\"Adelie\"\u001b[39m,\n",
      "  \u001b[32m\"Gentoo\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Gentoo\"\u001b[39m,    \u001b[32m\"Gentoo\"\u001b[39m,    \u001b[32m\"Chinstrap\"\u001b[39m,\n",
      "  \u001b[32m\"Gentoo\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Chinstrap\"\u001b[39m, \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Chinstrap\"\u001b[39m,\n",
      "  \u001b[32m\"Gentoo\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Gentoo\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Gentoo\"\u001b[39m,\n",
      "  \u001b[32m\"Gentoo\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Chinstrap\"\u001b[39m,\n",
      "  \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Gentoo\"\u001b[39m,    \u001b[32m\"Chinstrap\"\u001b[39m, \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Gentoo\"\u001b[39m,\n",
      "  \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Gentoo\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,\n",
      "  \u001b[32m\"Gentoo\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Gentoo\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,\n",
      "  \u001b[32m\"Gentoo\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Gentoo\"\u001b[39m,    \u001b[32m\"Chinstrap\"\u001b[39m, \u001b[32m\"Adelie\"\u001b[39m,\n",
      "  \u001b[32m\"Chinstrap\"\u001b[39m, \u001b[32m\"Chinstrap\"\u001b[39m, \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,\n",
      "  \u001b[32m\"Gentoo\"\u001b[39m,    \u001b[32m\"Chinstrap\"\u001b[39m, \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Gentoo\"\u001b[39m,\n",
      "  \u001b[32m\"Gentoo\"\u001b[39m,    \u001b[32m\"Gentoo\"\u001b[39m,    \u001b[32m\"Gentoo\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Gentoo\"\u001b[39m,\n",
      "  \u001b[32m\"Gentoo\"\u001b[39m,    \u001b[32m\"Gentoo\"\u001b[39m,    \u001b[32m\"Adelie\"\u001b[39m,    \u001b[32m\"Chinstrap\"\u001b[39m, \u001b[32m\"Gentoo\"\u001b[39m,\n",
      "  \u001b[32m\"Chinstrap\"\u001b[39m, \u001b[32m\"Gentoo\"\u001b[39m,    \u001b[32m\"Gentoo\"\u001b[39m,    \u001b[32m\"Gentoo\"\u001b[39m,    \u001b[32m\"Gentoo\"\u001b[39m\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "// Example 4: End-to-end with Table API\n",
    "\n",
    "// Split preserves all information\n",
    "const tableSplit = ds.ml.validation.trainTestSplit(\n",
    "  {\n",
    "    data: penguinsData,\n",
    "    X: numericColumns,\n",
    "    y: 'Species'\n",
    "  },\n",
    "  { ratio: 0.7, shuffle: true, seed: 42 }\n",
    ");\n",
    "\n",
    "// Fit scaler on training data\n",
    "const tableScaler = new ds.ml.preprocessing.StandardScaler()\n",
    "  .fit({\n",
    "    data: tableSplit.train.data,\n",
    "    columns: numericColumns\n",
    "  });\n",
    "\n",
    "// Transform both train and test (encoders pass through)\n",
    "const tableTrainScaled = tableScaler.transform({\n",
    "  data: tableSplit.train.data,\n",
    "  columns: numericColumns,\n",
    "  encoders: tableSplit.train.metadata.encoders\n",
    "});\n",
    "\n",
    "const tableTestScaled = tableScaler.transform({\n",
    "  data: tableSplit.test.data,\n",
    "  columns: numericColumns,\n",
    "  encoders: tableSplit.train.metadata.encoders  // Use TRAIN encoders\n",
    "});\n",
    "\n",
    "// Fit model (encoders ensure consistent label encoding)\n",
    "const tableKnn = new ds.ml.KNNClassifier({ k: 3 }).fit({\n",
    "  data: tableTrainScaled.data,\n",
    "  X: numericColumns,\n",
    "  y: 'Species',\n",
    "  encoders: tableTrainScaled.metadata.encoders\n",
    "});\n",
    "\n",
    "// Predict\n",
    "const tablePreds = tableKnn.predict({\n",
    "  data: tableTestScaled.data,\n",
    "  X: numericColumns,\n",
    "  encoders: tableTestScaled.metadata.encoders\n",
    "});\n",
    "\n",
    "console.log('Predictions:', tablePreds);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipe API\n",
    "\n",
    "The recipe API provides a chainable, declarative way to define preprocessing workflows. Akin to Posit R's recipe approach, the recipe API allows full inspection of intermediate results while ensuring transformers are correctly fitted and applied. Use it when\n",
    "\n",
    "- you have complex preprocessing with multiple steps,\n",
    "- you want to inspect intermediate transformations,\n",
    "- you need to apply the same preprocessing to new data,\n",
    "- you want to avoid manual encoder/metadata passing, or\n",
    "- you prefer verbose but readable, self-documenting workflows.\n",
    "\n",
    "The recipe API works with chainable methods defining preprocessing steps, with `prep()` executing all steps and returns inspectable results (important for intermediate verifications), and with `bake()` applying fitted transformers to new data. Transformers are stored and reusable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.0%\n"
     ]
    }
   ],
   "source": [
    "const numericFeatures = [\"Beak Length (mm)\", \"Beak Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\"];\n",
    "const preprocessRecipe = ds.ml.recipe({\n",
    "  data: penguinsData,\n",
    "    X: [ ... numericFeatures, 'Island', 'Sex'],\n",
    "    y: 'Species'\n",
    "  })\n",
    "  .oneHot(['Island', 'Sex'], { dropFirst: true })\n",
    "  .scale(numericFeatures)\n",
    "  .split({ ratio: 0.7, shuffle: true, seed: 42 });\n",
    "\n",
    "const prepped = preprocessRecipe.prep()\n",
    "\n",
    "const knnRecipe = new ds.ml.KNNClassifier({ k: 3 }).fit({\n",
    "  data: prepped.train.data,\n",
    "  X: prepped.train.X,\n",
    "  y: prepped.train.y,\n",
    "  encoders: prepped.train.metadata.encoders\n",
    "});\n",
    "\n",
    "const predictionsRecipe = knnRecipe.predict({\n",
    "  data: prepped.test.data,\n",
    "  X: prepped.test.X,\n",
    "  encoders: prepped.test.metadata.encoders\n",
    "});\n",
    "\n",
    "// Calculate accuracy\n",
    "const actual = prepped.test.data.map(d => d.Species);\n",
    "const correct = predictionsRecipe.filter((p, i) => p === actual[i]).length;\n",
    "console.log('Accuracy:', (correct / predictionsRecipe.length * 100).toFixed(1) + '%');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted species: Gentoo\n"
     ]
    }
   ],
   "source": [
    "// Example 4: Apply Recipe to New Data\n",
    "const newPenguin = [\n",
    "  { \n",
    "    \"Species\": \"Unknown\",\n",
    "    Island: \"Biscoe\", Sex: \"FEMALE\",\n",
    "    \"Beak Length (mm)\": 45.0, \"Beak Depth (mm)\": 15.0, \"Flipper Length (mm)\": 210, \"Body Mass (g)\": 4800\n",
    "  }\n",
    "];\n",
    "\n",
    "const newPrepped = preprocessRecipe.bake(newPenguin);// bake() applies all fitted transformers\n",
    "const newPrediction = model.predict({\n",
    "  data: newPrepped.data,\n",
    "  X: newPrepped.X\n",
    "});\n",
    "console.log('\\nPredicted species:', newPrediction[0]);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
